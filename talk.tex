\documentclass[xcolor=x11names]{beamer}
\usepackage{graphicx}
\usepackage{textpos}
\definecolor{light-gray}{gray}{0.6}

\usetheme{Frankfurt}

%\setbeamercolor{block title}{fg=white,bg=orange}
\setbeamercolor{frametitle}{fg=white, bg=orange}
\setbeamercolor{title}{fg=white, bg=orange}
\setbeamercolor{item projected}{fg=white,bg=orange}
\setbeamercolor{section in toc}{fg=black}
\setbeamercolor{bibliography entry title}{fg=black}
\setbeamercolor{bibliography entry author}{fg=black}
\setbeamertemplate{footline}[text line]{\parbox{\linewidth}{\vspace*{-8pt}
\thepage \hspace{0.35\textwidth} \textsc{\textcolor{light-gray}{Confidential \&
Proprietary}}}}
\setbeamertemplate{navigation symbols}{}


\logo{\includegraphics[height=0.8cm]{explorys_logo}}



\begin{document}

\title{Fast MapReduce over Apache HBase}
\subtitle{or How to Stop Worrying and Run MapReduce Directly Against Your HFiles}
\author{
	\large
	\flushleft
	{\bf Keith Wyss} \\
		{\em Software Engineer, Explorys Inc.} \\[0.25cm]
} 
\date{September, 24 2012} 

\frame{\titlepage}

\frame{\frametitle{Agenda} \tableofcontents}


\section{Introduction/Motivation}

\frame[t]{\frametitle{What is HBase}
  \begin{itemize}
    \item Hadoop's stock key value store.
  \end{itemize}
}

\frame[t]{\frametitle{What is HBase}
	\begin{itemize}
  		\item HBase is columnar, and fairly heterogenous. Why?
	\end{itemize}
}

\frame[t]{\frametitle{What is HBase}
	\begin{itemize}
  		\item HBase is columnar, and fairly heterogenous. Why?
                \item A mix of memory and disk: Storefiles, a MemStore and a Block Cache.
	\end{itemize}
}


\frame[t]{\frametitle{What is HBase}
	\begin{itemize}
  		\item HBase is columnar, and fairly heterogenous. Why?
                \item A mix of memory and disk: Storefiles, a MemStore and a Block Cache.
                \item Because of low-latency writes, files are not typed by column.
	\end{itemize}
}


\frame[t]{\frametitle{What is HBase}
	\begin{itemize}
  		\item HBase is columnar, and fairly heterogenous. Why?
                \item A mix of memory and disk: Storefiles, a MemStore and a Block Cache.
                \item Because of low-latency writes, files are not typed by column.
  		\item HBase is ``garbage collected'' and online-transformative.
	\end{itemize}
}
\frame[t]{\frametitle{More reasons to use HBase}
       \begin{itemize}
  		\item HBase provides a database that may be used a source for data processing via MapReduce. 
                  HBase makes it easy to perform batch processing at scale upon data that processes writes in real time.
                  Most attractive feature to some.
       \end{itemize}
}

\frame[t]{\frametitle{More reasons to use HBase}
       \begin{itemize}
  		\item HBase provides a database that may be used a source for data processing via MapReduce. 
                  HBase makes it easy to perform batch processing at scale upon data that processes writes in real time.
                  Most attractive feature to some.
  		\item HBase interoperates with the rest of the Apache distributed stack. It utilizes zookeeper efficiently to
                  spread the load of database operations evenly across a cluster if you structure the data properly.
       \end{itemize}
}

\frame[t]{\frametitle{More reasons to use HBase}
       \begin{itemize}
  		\item HBase provides a database that may be used a source for data processing via MapReduce. 
                  HBase makes it easy to perform batch processing at scale upon data that processes writes in real time.
                  Most attractive feature to some.
  		\item HBase interoperates with the rest of the Apache distributed stack. It utilizes zookeeper efficiently to
                  spread the load of database operations evenly across a cluster if you structure the data properly.
                \item HBase allows you to control your own destiny.
       \end{itemize}
}

\frame[t]{\frametitle{More reasons to use HBase}
       \begin{itemize}
  		\item HBase provides a database that may be used a source for data processing via MapReduce. 
                  HBase makes it easy to perform batch processing at scale upon data that processes writes in real time.
                  Most attractive feature to some.
  		\item HBase interoperates with the rest of the Apache distributed stack. It utilizes zookeeper efficiently to
                  spread the load of database operations evenly across a cluster if you structure the data properly.
                \item HBase allows you to control your own destiny.
                \item Interface is always bytes.
       \end{itemize}
}

\frame[t]{\frametitle{More reasons to use HBase}
       \begin{itemize}
  		\item HBase provides a database that may be used a source for data processing via MapReduce. 
                  HBase makes it easy to perform batch processing at scale upon data that processes writes in real time.
                  Most attractive feature to some.
  		\item HBase interoperates with the rest of the Apache distributed stack. It utilizes zookeeper efficiently to
                  spread the load of database operations evenly across a cluster if you structure the data properly.
                \item HBase allows you to control your own destiny.
                \item Interface is always bytes.
                \item Lots of control. No mandatory schemas or serialization of data types.
       \end{itemize}
}


\frame[t]{\frametitle{Why MapReduce Over HBase Detracts from Both}
      \begin{itemize}
      \item Running MapReduce over an HBase table using the TableInputFormat is slow. It is many times slower than processing the same data
        sourced from raw HDFS files.
      \end{itemize}
}
\frame[t]{\frametitle{Why MapReduce Over HBase Detracts from Both}
      \begin{itemize}
      \item Running MapReduce over an HBase table using the TableInputFormat is slow. It is many times slower than processing the same data
        sourced from raw HDFS files.
      \item Three interfering use cases: read, write, and batch.
      \end{itemize}
}
\frame[t]{\frametitle{Why MapReduce Over HBase Detracts from Both}
      \begin{itemize}
      \item Running MapReduce over an HBase table using the TableInputFormat is slow. It is many times slower than processing the same data
        sourced from raw HDFS files.
      \item Three interfering use cases: read, write, and batch.
      \item Running MapReduce over an HBase table will destroy your read performance. The scanners that the MapReduce job employs flood the block
        cache with data that does not reflect your random read access patterns. Also, be prepared to deal with thrashing as your HBase table is probably
        quite large and none of it will be read twice, so none of this cache management is beneficial.
      \end{itemize}
}

\frame[t]{\frametitle{Why MapReduce Over HBase Detracts from Both}
      \begin{itemize}
      \item Running MapReduce over an HBase table using the TableInputFormat is slow. It is many times slower than processing the same data
        sourced from raw HDFS files.
      \item Three interfering use cases: read, write, and batch.
      \item Running MapReduce over an HBase table will destroy your read performance. The scanners that the MapReduce job employs flood the block
        cache with data that does not reflect your random read access patterns unless you explicitly turn them off. Also, be prepared to deal with thrashing as your HBase table is probably
        quite large and none of it will be read twice, so none of this cache management is beneficial.
      \item What is happening with a typical Scan?
      \end{itemize}
}
\frame[t]{\frametitle{Architecture of an HBase Scan \cite{LINELAND}}
     \includegraphics[height=6cm]{hbase-files}
}
\frame[t]{\frametitle{Running the Diagnostics}
      \begin{itemize}
        \item Can we break the problems with running MapReduce over HBase down to a common root.
      \end{itemize}
}
\frame[t]{\frametitle{Running the Diagnostics}
      \begin{itemize}
        \item Can we break the problems with running MapReduce over HBase down to a common root.
        \item The batch processing data access pattern is conflated with a typical client read scenario. MapReduce (TableInputFormat) data access is naive.
      \end{itemize}
}
\frame[t]{\frametitle{Running the Diagnostics}
      \begin{itemize}
        \item Can we break the problems with running MapReduce over HBase down to a common root.
        \item The batch processing data access pattern is conflated with a typical client read scenario. MapReduce (TableInputFormat) data access is naive.
        \item Not naive as in dumb. Naive because of code re-use.
      \end{itemize}
}
\frame[t]{\frametitle{Running the Diagnostics}
      \begin{itemize}
        \item Can we break the problems with running MapReduce over HBase down to a common root.
        \item The batch processing data access pattern is conflated with a typical client read scenario. MapReduce (TableInputFormat) data access is naive.
        \item Not naive as in dumb. Naive because of code re-use.
        \item Each map task uses the client API to scan over the Results in the HBase table. The primary advantage over typical use is
          merely concurrent scans allowing parallelism. The overhead of the client server architecture is offset by a large amount of
          buffering to reduce the impact of Remote Procedure calls, but it is far from the mark of a direct file-system access even if the data is
          local to the MapReduce task.
      \end{itemize}
}

\section{Design}
\frame[t]{\frametitle{Direct Access}
  \begin{itemize}
    \item What do we want instead?

  \end{itemize}
}

\frame[t]{\frametitle{Direct Access}
  \begin{itemize}
    \item What do we want instead?
    \item An InputFormat that allows direct, read-only subscription to the StoreFiles for an HBase Table.
  \end{itemize}
}



\frame[t]{\frametitle{Direct Access}
  \begin{itemize}
    \item What do we want instead?
    \item An InputFormat that allows direct, read-only subscription to the StoreFiles for an HBase Table.
    \item Through some conversation with the HBase committers, especially St. Ack, we were told
      \begin{enumerate}
        \item We were crazy for thinking of this.
        \item More direct data access is something that's been requested before.
        \item We might have a viable approach by spinning up our own Region objects and using those to gain access to the underlying HFiles.
      \end{enumerate}
    \item There are a few contraindications to the approach that we will discuss later.
  \end{itemize}
}

\frame[t]{\frametitle{How to make this fast}
  \begin{itemize}
    \item Each storeFile is a column-family specific sorted list of KeyValues. The storefiles themselves are ordered by the timestamps.
  \end{itemize}
}

\frame[t]{\frametitle{How to make this fast}
  \begin{itemize}
    \item Each storeFile is a column-family specific sorted list of KeyValues. The storefiles themselves are ordered by the timestamps.
    \item The approach is to treat each HFile as a heap of KeyValues. We then built an outer heap of HFiles where the comparator is
      the KeyValue.COMPARATOR applied to the top element of the heap.
  \end{itemize}
}

\frame[t]{\frametitle{How to make this fast}
  \begin{itemize}
    \item Each storeFile is a column-family specific sorted list of KeyValues. The storefiles themselves are ordered by the timestamps.
    \item The approach is to treat each HFile as a heap of KeyValues. We then built an outer heap of HFiles where the comparator is
      the KeyValue.COMPARATOR applied to the top element of the heap.
    \item Then we merge the results into a Result object as they are read in to provide the familiar pair of rowkey and Result object.
  \end{itemize}
}


\frame[t]{\frametitle{How to make this fast}
  \begin{itemize}
    \item Each storeFile is a column-family specific sorted list of KeyValues. The storefiles themselves are ordered by the timestamps.
    \item The approach is to treat each HFile as a heap of KeyValues. We then built an outer heap of HFiles where the comparator is
      the KeyValue.COMPARATOR applied to the top element of the heap.
    \item Then we merge the results into a Result object as they are read in to provide the familiar pair of rowkey and Result object.
    \item Back to online-transformation again. Tombstone markers handle with reverse-timestamp sorting.
  \end{itemize}
}

\frame[t]{\frametitle{Anatomy of an HFile \cite{SCHUBERT09,HBASEIO11}}
  \begin{columns}[t]
  \begin{column}{5cm}
  \begin{tabular}{|l|l|}
    \hline
    \multicolumn{2}{|c|}{A Typical HFile} \\
    \hline
   1 & Data Block 0 \\
    \hline
   2 &  Data Block 1 \\
    \hline
   3 & Meta Block \\
    \hline
   4 & File Info \\
    \hline
   5 & Data Block Index \\
    \hline
   6 & Meta Block Index \\
    \hline
   7 & Trailer \\
    \hline
  \end{tabular}
  \end{column}
  \begin{column}{5cm}
  \begin{tabular}{|l|l|}
    \hline
    \multicolumn{2}{|c|}{A Typical DataBlock} \\
    \hline
   1 & Data Block Magic \\
    \hline
   2 & Key Length \\
    \hline
   3 & Value Length \\
    \hline
   4 & Key \\
    \hline
   5 & Value \\
    \hline
   6 & Key Length \\
    \hline
   7 & Value Length \\
    \hline
   8 & Key \\
    \hline
   9 & Value \\
    \hline
   10 & ... \\
    \hline
   \end{tabular}
  \end{column}
  \end{columns}
}

\section{Operation}
\frame[t]{\frametitle{Implementation}
  \begin{itemize}
    \item Here at Explorys, we implemented this proposed InputFormat to be used as a plug and play replacement for TableInputFormat.
    \item Almost...
    \item The code is hosted on github at \href{https://github.com/ExplorysMedical/Apothecary}{https://github.com/ExplorysMedical/Apothecary}
    \item Problems? Compaction and Region Splitting. Memstore access. Why?
  \end{itemize}
}


\frame[t]{\frametitle{Dealing with HBase Data Administration}

 \begin{enumerate}
    \item Turn off data source or funnel into temporary storage.
    \item Trigger flush and compaction.
    \item Wait for flush and compaction to finish. All the data is on disk.
    \item Create a log of the existing storefiles.
    \item Run the MapReduce Job
    \item Check the log to see that the state of the table didn't change (region splits or compaction).
    \item Possible do-over.
  \end{enumerate}
  \vspace{0.2cm}
}

\frame[t]{\frametitle{Dealing with HBase Data Administration}

 \begin{enumerate}
    \item Turn off data source or funnel into temporary storage.
    \item Trigger flush and compaction.
    \item Wait for flush and compaction to finish. All the data is on disk.
    \item Create a log of the existing storefiles.
    \item Run the MapReduce Job
    \item Check the log to see that the state of the table didn't change (region splits or compaction).
    \item Possible do-over.
  \end{enumerate}
  \vspace{0.2cm}
 \begin{itemize}
  \item That's a lot. What can we skip?
  \end{itemize}
}

\frame[t]{\frametitle{Dealing with HBase Data Administration}

 \begin{enumerate}
    \item Turn off data source or funnel into temporary storage.
    \item Trigger flush and compaction.
    \item Wait for flush and compaction to finish. All the data is on disk.
    \item Create a log of the existing storefiles.
    \item Run the MapReduce Job
    \item Check the log to see that the state of the table didn't change (region splits or compaction).
    \item Possible do-over.
  \end{enumerate}
  \vspace{0.2cm}
 \begin{itemize}
  \item That's a lot. What can we skip?
  \item Hard links?
  \end{itemize}
}

\frame[t]{\frametitle{Performance}
   \begin{tabular}{|l|l|l|}
     \hline
      & TableInputFormat & HFiles \\
     \hline
     Copying a Table & 5:06 & 0:26 \\
     \hline
     Explorys Indexing Job & 1:50 & 0:30 \\
     \hline
   \end{tabular}
   \vspace{2cm}
 }

\frame[t]{\frametitle{Performance}
   \begin{tabular}{|l|l|l|}
     \hline
      & TableInputFormat & HFiles \\
     \hline
     Copying a Table & 5:06 & 0:26 \\
     \hline
     Explorys Indexing Job & 1:50 & 0:30 \\
     \hline
   \end{tabular}
   \vspace{2cm}
   \begin{itemize}
     \item Indexing Job run with 800 input splits, about 300 concurrent mappers.
     \item Copy Job run with 800 input splits, 95 concurrent Mappers.
     \item Significantly less cluster resources allocated to HBase on Copy Job.
   \end{itemize} 
 }

\frame[t]{\frametitle{Performance}
   \begin{tabular}{|l|l|l|}
     \hline
      & TableInputFormat & HFiles \\
     \hline
     Copying a Table & 5:06 & 0:26 \\
     \hline
     Explorys Indexing Job & 1:50 & 0:30 \\
     \hline
   \end{tabular}
   \vspace{2cm}
   \begin{itemize}
     \item Better over large tables where cache doesn't have an impact.
     \item Worth the operation overhead in some circumstances.
     \item Major motivation for hard links and/or table snapshotting.
   \end{itemize} 
 }

\frame[t]{\frametitle{Performance}
   \begin{tabular}{|l|l|l|}
     \hline
      & TableInputFormat & HFiles \\
     \hline
     Copying a Table & 5:06 & 0:26 \\
     \hline
     Explorys Indexing Job & 1:50 & 0:30 \\
     \hline
   \end{tabular}
   \vspace{2cm}
   \begin{itemize}
     \item Due to a bug scanner caching for the control group was accidentally turned off.
   \end{itemize} 
 }


\frame[t]{\frametitle{Performance}
   \begin{tabular}{|l|l|l|}
     \hline
      & TableInputFormat & HFiles \\
     \hline
     Copying a Table & 0:33 & 0:26 \\
     \hline
   \end{tabular}
   \vspace{2cm}
  \begin{itemize}
   \item Still might be beneficial if tables are constructed using bulk generation.
   \item RPC calls were not as big of a deal as initially thought.
  \end{itemize}
 }


\frame[t]{\frametitle{Problems}
  \vspace{0.5cm}
  \begin{itemize}
    \item Serious memory usage. HFile block indexes can be quite large, although needn't be loaded in full since HBase 0.92 \cite{HFILEV2}
    \item Operational constraints unwieldy. HBase Administration is asynchronous, doesn't return monitoring object.
    \item Tenuous relationship with non-exposed API. No guarantees that upgrades won't break it.
    \item Not tested with filesystems other than HDFS.
    \item Aside from compaction rearranging files, Memstore is not included. 
      We could possibly read WAL and construct our own memstore if compaction problem is solved.
  \end{itemize}
}


\frame[t,allowframebreaks]{\frametitle{References}
\bibliographystyle{amsalpha}
\begin{thebibliography}{1}
	{\small
          \bibitem{SCHUBERT09}
            Schubert Zhang (2009). HFile: A Block-Indexed File Format for Store Sorted Key-Value Pairs
            \href{http://www.slideshare.net/schubertzhang/hfile-a-blockindexed-file-format-to-store-sorted-keyvalue-pairs}{http://www.slideshare.net/schubertzhang/hfile-a-blockindexed-file-format-to-store-sorted-keyvalue-pairs}
          \bibitem{HBASEIO11}
            Matteo Bertozzi (2011). HBase I/O: HFile
            \href{http://th30z.blogspot.com/2011/02/hbase-io-hfile.html?spref=tw}{http://th30z.blogspot.com/2011/02/hbase-io-hfile.html?spref=tw}
          \bibitem{HFILEV2}
            Apache Software Foundation (2012). HBase Book: Appendix E. HFile format version 2
            \href{http://hbase.apache.org/book/hfilev2.html}{http://hbase.apache.org/book/hfilev2.html}
          \bibitem{LINELAND}
            Lars George (2009). HBase Architecture 101 -Storage
            \href{http://www.larsgeorge.com/2009/10/hbase-architecture-101-storage.html}{http://www.larsgeorge.com/2009/10/hbase-architecture-101-storage.html}
	}
\end{thebibliography}
}
\end{document}
